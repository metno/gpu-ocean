{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "This notebook sets up and runs a set of benchmarks to compare\n",
    "different numerical discretizations of the SWEs\n",
    "\n",
    "Copyright (C) 2016  SINTEF ICT\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU General Public License\n",
    "along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Particle Filter\n",
    "\n",
    "In this notebook we will make test implementations of basic particle filters.\n",
    "\n",
    "The aim is to find a decent implementation of the particles, which can be used by both simulators and particle filter.\n",
    "\n",
    "All post-processing of particles will be done on the CPU in this first iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of basic resampling strategies\n",
    "\n",
    "The following four resampling algorithms are described in Section 3a) in van Leeuven's 2009 review paper.\n",
    "\n",
    "The starting point is a prior distribution of model states with pdf $p(\\psi^0)$, from which $N$ model state samples (particles) $\\psi_i^0$, $i = 1,...,N$ are drawn.\n",
    "Run the simulation model on all particles $\\psi_i^n = f(\\psi_i^{n-1})$. This is the same as sampling from the $p(\\psi^n | \\psi_i^{n-1})$.\n",
    "At this point we see an observation $d$.\n",
    "\n",
    "Now, define a posterior distribution $p(\\psi^n | d)$. **Think a bit here before writing more** - from this we obtain weights $w_i$.\n",
    "\n",
    "\n",
    "The most basic resampling strategies are as follows:\n",
    "### Probabilistic resampling\n",
    "Use the weights as a discrete distribution and sample directly from this.\n",
    "\n",
    "### Residual sampling\n",
    "Here, we first resample particles deterministic based on their weights. Resample particle $i$ `np.floor`$(Nw_i)$ times. Define the left-over weights as $w^*_i = Nw_i \\% 1$, and use $w^*_i$ as a discrete distribution, from which the reminder resampled particles are drawn from, until we have $N$ particles again.\n",
    "\n",
    "**Note:** It is here recommended to use Cauchy distribution as the observation pdf.\n",
    "\n",
    "### Stochastic Universal sampling\n",
    "Put all weights as buckets on the line $[0, 1]$, and draw a random number $u \\sim U[0, 1/N]$.\n",
    "Put $N$ line pieces starting from $u$ with length $1/N$ are laid on the line $[0,1]$. \n",
    "The bucket in which each line piece ends (not started???) defines which particle is chosen for each of the $N$ line pieces.\n",
    "\n",
    "### Monte Carlo Metropolis-Hastings sampling\n",
    "This sampling scheme is described very algorithmic in the paper already, so take a look at 3a4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules and set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lets have matplotlib \"inline\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#Import packages we need\n",
    "import numpy as np\n",
    "from matplotlib import animation, rc\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "import os\n",
    "import pyopencl\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "#Set large figure sizes\n",
    "rc('figure', figsize=(16.0, 12.0))\n",
    "rc('animation', html='html5')\n",
    "\n",
    "#Import our simulator\n",
    "from SWESimulators import CTCS, CDKLM16, PlotHelper, Common\n",
    "#Import initial condition and bathymetry generating functions:\n",
    "from SWESimulators.BathymetryAndICs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make sure we get compiler output from OpenCL\n",
    "os.environ[\"PYOPENCL_COMPILER_OUTPUT\"] = \"1\"\n",
    "\n",
    "#Set which CL device to use, and disable kernel caching\n",
    "if (str.lower(sys.platform).startswith(\"linux\")):\n",
    "    os.environ[\"PYOPENCL_CTX\"] = \"0\"\n",
    "else:\n",
    "    os.environ[\"PYOPENCL_CTX\"] = \"1\"\n",
    "os.environ[\"CUDA_CACHE_DISABLE\"] = \"1\"\n",
    "os.environ[\"PYOPENCL_COMPILER_OUTPUT\"] = \"1\"\n",
    "os.environ[\"PYOPENCL_NO_CACHE\"] = \"1\"\n",
    "\n",
    "#Create OpenCL context\n",
    "cl_ctx = pyopencl.create_some_context()\n",
    "print \"Using \", cl_ctx.devices[0].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts on code structure\n",
    "\n",
    "The observation will in these initial cases be a chosen model realization. When initializing a data assimilation with N particles, N+1 particles should be created and distributed on simulators.\n",
    "\n",
    "One hypothesis for our ocean simulator is that integrating 100 particles within the same simulation is equally expensive as integrating 1 particle. Each particle integration should be done with a single thread on the GPU, so all 100 particles will can be processed in parallel.\n",
    "\n",
    "If this assumption is true, it is best to have all particle positions continuous in memory. Hence, it will be implemented as an struct of array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create random particles, and create random observation\n",
    "\n",
    "Particles are created in a GlobalParticle class, which holds the positions of all ensemble member particles, and one additional particle which serves as our observation.\n",
    "This class should have all computational functionality that relies on the relationships between particles and the observation, and the ensemble itself.\n",
    "\n",
    "Filtering and resampling of particles does not belong in this class.\n",
    "\n",
    "List of functions:\n",
    "- [x] initialize uniform on unit square\n",
    "- [ ] initialize gaussian \n",
    "- [ ] Calculate distances from observation\n",
    "- [x] get weights from Gaussian distribution\n",
    "- [x] get weights from Cauchy distribution\n",
    "- [ ] find ensemble mean position\n",
    "- [ ] find ensemble variance\n",
    "- [ ] set observation to a given coordinate\n",
    "\n",
    "**About distances**: In order to calculate distances in a unified way, information about boundary conditions needs to be known by the class. E.g., the distance between a particle at $(0.99, 0.99)$ from an observation at $(0.01, 0.02)$ on a unit square domain, is about $\\sqrt{2}$. However, with periodic boundary conditions, their distance is only $0.05$.\n",
    "\n",
    "Let the domain size be $(L_x, L_y)$, and define the particle and observation positions as $(x_p, y_p)$ and $(x_o, y_o)$, respectively.\n",
    "The minimal distance with a periodic boundary can then be found by\n",
    "$$ d_{x,min} = \\min \\left\\{ |x_p - x_o|, |(x_p - L_x) - x_o|, |(x_p + L_x) - x_o| \\right\\}$$\n",
    "$$ d_{y,min} = \\min \\left\\{ |y_p - y_o|, |(y_p - L_y) - y_o|, |(y_p + L_y) - y_o| \\right\\}$$\n",
    "and \n",
    "$$ d_{min} = \\sqrt{ d_{x,min}^2 + d_{y,min}^2}$$\n",
    "\n",
    "** About ensemble mean position**: When finding the mean position of particles, the above considerations needs to be taken as well. The ensemble mean position should be found by the coordinate position which results in the minimal distance given above.\n",
    "In other words, the position we should consider for the mean is\n",
    "$$ x^*_p = {\\arg\\min}_{x \\in \\{x_p, x_p \\pm L_x \\}} |x - x_o|, $$\n",
    "$$ y^*_p = {\\arg\\min}_{y \\in \\{y_p, y_p \\pm L_y \\}} |y - y_o|. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seed so that all simulation runs are equal.\n",
    "np.random.seed(10)\n",
    "\n",
    "print np.zeros((2,3))\n",
    "a = np.random.rand(3,2)\n",
    "print a, a.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### GlobalParticlesClass\n",
    "\n",
    "class GlobalParticles:\n",
    "    def __init__(self, numParticles, observation_variance=0.1, boundaryConditions=Common.BoundaryConditions()):\n",
    "        \n",
    "        self.numParticles = numParticles\n",
    "        \n",
    "        # Observation index is the last particle\n",
    "        self.obs_index = self.numParticles \n",
    "        self.observation_variance = observation_variance\n",
    "        \n",
    "        # One position for every particle plus observation\n",
    "        self.positions = np.zeros((self.numParticles + 1, 2))\n",
    "        \n",
    "        self.domain_size_x = 1.0\n",
    "        self.domain_size_y = 1.0\n",
    "        \n",
    "        # Boundary conditions are read from a BoundaryConditions object\n",
    "        self.boundaryConditions = boundaryConditions\n",
    "        \n",
    "    def initializeInSquare(self, domain_size_x=1.0, domain_size_y=1.0):\n",
    "        \n",
    "        # Initialize in unit square\n",
    "        self.positions = np.random.rand(self.numParticles + 1, 2)\n",
    "        # Ensure that the observation is in the middle 0.5x0.5 square:\n",
    "        self.positions[self.obs_index, :] = self.positions[self.obs_index]*0.5 + 0.25\n",
    "        \n",
    "        # Map to given square\n",
    "        self.positions[:,0] = self.positions[:,0]*domain_size_x\n",
    "        self.positions[:,1] = self.positions[:,1]*domain_size_y\n",
    "        \n",
    "        self.domain_size_x = domain_size_x\n",
    "        self.domain_size_y = domain_size_y\n",
    "        \n",
    "    \"\"\"\n",
    "    Returns a set of coordinates corresponding to each particles closest position to the observation,\n",
    "    considering possible periodic boundary conditions\n",
    "    \"\"\"\n",
    "    def _getClosestPositions(self):\n",
    "        if not (self.boundaryConditions.isPeriodicNorthSouth() or self.boundaryConditions.isPeriodicEastWest()):\n",
    "            return self.positions\n",
    "        else:\n",
    "            periodicPositions = self.positions.copy()\n",
    "            obs_x, obs_y = periodicPositions[self.obs_index, :]\n",
    "            if self.boundaryConditions.isPeriodicEastWest():\n",
    "                for i in range(self.numParticles):\n",
    "                    x = periodicPositions[i,0]\n",
    "                    \n",
    "                    pos_x = np.array([x - self.domain_size_x, x, x + self.domain_size_x])\n",
    "                    dist_x = np.abs(pos_x - obs_x)\n",
    "                    periodicPositions[i,0] = pos_x[np.argmin(dist_x)]\n",
    "\n",
    "            if self.boundaryConditions.isPeriodicNorthSouth():\n",
    "                for i in range(self.numParticles):\n",
    "                    y = periodicPositions[i,1]\n",
    "                    \n",
    "                    pos_y = np.array([y - self.domain_size_y, y, y + self.domain_size_y])\n",
    "                    dist_y = np.abs(pos_y - obs_y)\n",
    "                    periodicPositions[i,1] = pos_y[np.argmin(dist_y)]\n",
    "        return periodicPositions\n",
    "        \n",
    "        \n",
    "    def getDistances(self):\n",
    "        distances = np.zeros(self.numParticles)\n",
    "        closestPositions = self._getClosestPositions()\n",
    "        obs_x, obs_y = self.positions[self.obs_index, :]\n",
    "        for i in range(self.numParticles):\n",
    "            distances[i] = np.sqrt( (closestPositions[i,0]-obs_x)**2 +\n",
    "                                    (closestPositions[i,1]-obs_y)**2)\n",
    "        return distances\n",
    "        \n",
    "    def getParticlePositions(self):\n",
    "        return self.positions[:-1,:]\n",
    "    \n",
    "    def getObservationPosition(self):\n",
    "        return self.positions[-1, :]\n",
    "    \n",
    "    def getGaussianWeight(self, distance=None, normalize=True):\n",
    "        if distance is None:\n",
    "            distance = self.getDistances()\n",
    "        weights = (1.0/np.sqrt(2*np.pi*self.observation_variance**2))* \\\n",
    "            np.exp(- (distance**2/(2*self.observation_variance**2)))\n",
    "        if normalize:\n",
    "            return weights/np.sum(weights)\n",
    "        return weights\n",
    "    \n",
    "    \"\"\"\n",
    "    Weights are calculated using a Cauchy Distribution.\n",
    "    It is chosen over a Gauss distribution in order to obtain wider tails.\n",
    "    \"\"\"\n",
    "    def getCauchyWeight(self, distance=None, normalize=True):\n",
    "        if distance is None:\n",
    "            distance = self.getDistances()\n",
    "        weights = 1.0/(np.pi*self.observation_variance*(1 + (distance/self.observation_variance)**2))\n",
    "        if normalize:\n",
    "            return weights/np.sum(weights)\n",
    "        return weights    \n",
    "    \n",
    "    def plotDistanceInfo(self, title=None):\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        gridspec.GridSpec(2, 3)\n",
    "        \n",
    "        # PLOT POSITIONS OF PARTICLES AND OBSERVATIONS\n",
    "        ax0 = plt.subplot2grid((2,3), (0,0))\n",
    "        plt.plot(self.getParticlePositions()[:,0], \\\n",
    "                 self.getParticlePositions()[:,1], 'b.')\n",
    "        plt.plot(self.getObservationPosition()[0], \\\n",
    "                 self.getObservationPosition()[1], 'r.')\n",
    "        plt.xlim(0, self.domain_size_x)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.ylim(0, self.domain_size_y)\n",
    "        plt.title(\"Particle positions\")\n",
    "        \n",
    "        # PLOT DISCTRIBUTION OF PARTICLE DISTANCES AND THEORETIC OBSERVATION PDF\n",
    "        ax0 = plt.subplot2grid((2,3), (0,1), colspan=2)\n",
    "        distances = self.getDistances()\n",
    "        plt.hist(distances, bins=30, range=(0, max(min(self.domain_size_x, self.domain_size_y), np.max(distances))),\\\n",
    "                 normed=True, label=\"particle distances\")\n",
    "        \n",
    "        # With observation \n",
    "        x = np.linspace(0, max(self.domain_size_x, self.domain_size_y), num=100)\n",
    "        cauchy_pdf = self.getCauchyWeight(x, normalize=False)\n",
    "        gauss_pdf = self.getGaussianWeight(x, normalize=False)\n",
    "        plt.plot(x, cauchy_pdf, 'r', label=\"obs Cauchy pdf\")\n",
    "        plt.plot(x, gauss_pdf, 'g', label=\"obs Gauss pdf\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Distribution of particle distances from observation\")\n",
    "        \n",
    "        # PLOT SORTED DISTANCES FROM OBSERVATION\n",
    "        ax0 = plt.subplot2grid((2,3), (1,0), colspan=3)\n",
    "        cauchyWeights = self.getCauchyWeight(distances)\n",
    "        gaussWeights = self.getGaussianWeight(distances)\n",
    "        indices_sorted_by_observation = distances.argsort()\n",
    "        plt.plot(distances[indices_sorted_by_observation], label=\"distance\")\n",
    "        plt.plot(cauchyWeights[indices_sorted_by_observation]/np.max(cauchyWeights), 'r', label=\"Cauchy weight\")\n",
    "        plt.plot(gaussWeights[indices_sorted_by_observation]/np.max(gaussWeights), 'g', label=\"Gauss weight\")\n",
    "        plt.title(\"Sorted distances from observation\")\n",
    "        plt.grid()\n",
    "        plt.ylim(0,1.4)\n",
    "        plt.legend()\n",
    "        if title is not None:\n",
    "            plt.suptitle(title, fontsize=16)\n",
    "\n",
    "# Initialize an ensemble of particles:\n",
    "N = 100\n",
    "observation_variance = 0.08\n",
    "resample_variance = 0.005\n",
    "bc = 2\n",
    "boundaryConditions = Common.BoundaryConditions(bc,bc,bc,bc)\n",
    "globalParticles = GlobalParticles(N, observation_variance, boundaryConditions=boundaryConditions)\n",
    "globalParticles.initializeInSquare()\n",
    "\n",
    "# The particles are by now drawn from the prior distribution\n",
    "\n",
    "# Here, the simulation/time integration should take place\n",
    "\n",
    "# Inspect initial ensemble\n",
    "globalParticles.plotDistanceInfo(title=\"Initial particles\")\n",
    "print \"Observation: \", globalParticles.getObservationPosition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Testing boundary conditions\n",
    "test_N = 3\n",
    "far   = \"far      \"\n",
    "close = \"close    \"\n",
    "test_solutions = np.array([[[far, far, far], [far, close, far]], [[far, far, close], [close, close, close]]])\n",
    "for test_bc_ns in [1, 2]:\n",
    "    for test_bc_ew in [1, 2]:\n",
    "        test_boundaryConditions = Common.BoundaryConditions(test_bc_ns, test_bc_ew, test_bc_ns, test_bc_ew)\n",
    "        test_particles = GlobalParticles(test_N, boundaryConditions=test_boundaryConditions)\n",
    "        test_particles.positions[0,:] = [0.9, 0.9]\n",
    "        test_particles.positions[1,:] = [0.9, 0.1]\n",
    "        test_particles.positions[2,:] = [0.1, 0.9]\n",
    "        test_particles.positions[3,:] = [0.1, 0.1]\n",
    "        print \"(test_bc_ns, test_bc_ew)\", (test_bc_ns, test_bc_ew)\n",
    "        print test_particles.getDistances()\n",
    "        print test_solutions[test_bc_ns-1, test_bc_ew-1, :]\n",
    "        print \"--------------------\"\n",
    "print test_particles.positions\n",
    "#test_particles.plotDistanceInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "globalParticles.plotDistanceInfo(title=\"Initial particles\")\n",
    "\n",
    "# Create a new GlobalParticles instance based on newSampleIndices\n",
    "def resampleParticles(particles, newSampleIndices, reinitialization_variance):\n",
    "    newParticles = GlobalParticles(len(newSampleIndices))\n",
    "    if particles.numParticles != newParticles.numParticles:\n",
    "        print \"WARNING: The size of the new ensemble differs from the old size!\"\n",
    "        print \"(old size, new size): \", (particles.numParticles, newParticles.numParticles)\n",
    "    \n",
    "    # We really do not the if. The random number with zero variance returns exactly the mean\n",
    "    if reinitialization_variance == 0:\n",
    "        # Simply copy the given positions\n",
    "        newParticles.positions[:-1,:] = particles.positions[newSampleIndices,:].copy()\n",
    "    else:\n",
    "        # Make sure to make a clean copy of first resampled particle, and add a disturbance of the next ones.\n",
    "        resampledOnce = np.full(particles.numParticles, False, dtype=bool)\n",
    "        var = np.eye(2)*reinitialization_variance\n",
    "        for i in range(len(newSampleIndices)):\n",
    "            index = newSampleIndices[i]\n",
    "            if resampledOnce[index]:\n",
    "                newParticles.positions[i,:] = np.random.multivariate_normal(particles.positions[index,:], var)\n",
    "            else:\n",
    "                newParticles.positions[i,:] = particles.positions[index,:]\n",
    "                resampledOnce[index] = True\n",
    "                                                                            \n",
    "        \n",
    "        \n",
    "    # Copy the observation:\n",
    "    newParticles.positions[-1,:] = particles.positions[-1,:]\n",
    "    \n",
    "    return newParticles\n",
    "    \n",
    "\n",
    "\n",
    "def probabilisticResampling(particles, reinitialization_variance=0):\n",
    "    # Obtain weights:\n",
    "    weights = particles.getGaussianWeight()\n",
    "    #weights = particles.getCauchyWeight()\n",
    "    \n",
    "    # Create array of possible indices to resample:\n",
    "    allIndices = range(particles.numParticles)\n",
    "    \n",
    "    # Draw new indices based from discrete distribution based on weights\n",
    "    newSampleIndices = np.random.choice(allIndices, particles.numParticles, p=weights)\n",
    "        \n",
    "    # Return a new set of particles\n",
    "    return resampleParticles(particles, newSampleIndices, reinitialization_variance)\n",
    "\n",
    "probabilisticResampledParticles = probabilisticResampling(globalParticles, reinitialization_variance=0.0)\n",
    "probabilisticResampledParticles.plotDistanceInfo(title=\"From probabilistic resampling (identical resampling)\")\n",
    "\n",
    "probabilisticResampledParticles = probabilisticResampling(globalParticles, reinitialization_variance=resample_variance)\n",
    "probabilisticResampledParticles.plotDistanceInfo(title=\"From probabilistic resampling (gaussian resampling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    " \n",
    "def residualSampling(particles, reinitialization_variance=0, onlyDeterministic=False, onlyStochastic=False):\n",
    "    # Obtain weights:\n",
    "    #weights = particles.getCauchyWeight()\n",
    "    weights = particles.getGaussianWeight()\n",
    "\n",
    "    # Create array of possible indices to resample:\n",
    "    allIndices = range(particles.numParticles)\n",
    "\n",
    "    # Deterministic resampling based on the integer part of N*weights:\n",
    "    weightsTimesN = weights*particles.numParticles\n",
    "    weightsTimesNInteger = np.int64(np.floor(weightsTimesN))\n",
    "    deterministicResampleIndices = np.repeat(allIndices, weightsTimesNInteger)\n",
    "    \n",
    "    # Stochastic resampling based on the decimal parts of N*weights:\n",
    "    decimalWeights = np.mod(weightsTimesN, 1)\n",
    "    decimalWeights = decimalWeights/np.sum(decimalWeights)\n",
    "    stochasticResampleIndices = np.random.choice(allIndices, \n",
    "                                                 particles.numParticles - len(deterministicResampleIndices), \n",
    "                                                 p=decimalWeights)\n",
    "    ### NOTE!\n",
    "    # In numpy v >= 1.13, np.divmod can be used to get weightsTimesNInteger and decimalWeights from one function call.\n",
    "    \n",
    "    if onlyDeterministic:\n",
    "        return resampleParticles(particles, deterministicResampleIndices, reinitialization_variance)\n",
    "    if onlyStochastic:\n",
    "        return resampleParticles(particles, stochasticResampleIndices, reinitialization_variance)\n",
    "    \n",
    "    return resampleParticles(particles, np.concatenate((deterministicResampleIndices, stochasticResampleIndices)), \\\n",
    "                             reinitialization_variance)\n",
    "    \n",
    "globalParticles.plotDistanceInfo(title=\"Initial particles\")\n",
    "\n",
    "\n",
    "\n",
    "residualSamplingParticles = residualSampling(globalParticles, reinitialization_variance=resample_variance)\n",
    "residualSamplingParticles.plotDistanceInfo(title=\"From residual sampling\")\n",
    "\n",
    "residualSamplingParticlesDet = residualSampling(globalParticles, reinitialization_variance=resample_variance, \\\n",
    "                                                onlyDeterministic=True)\n",
    "residualSamplingParticlesDet.plotDistanceInfo(title=\"From residual sampling - Deterministic part only\")\n",
    "\n",
    "residualSamplingParticlesStoc = residualSampling(globalParticles, reinitialization_variance=resample_variance,\\\n",
    "                                                 onlyStochastic=True)\n",
    "residualSamplingParticlesStoc.plotDistanceInfo(title=\"From residual sampling - Stochastic part only\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stochastic Universal Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stochasticUniversalSampling(particles, reinitialization_variance=0):\n",
    "    # Obtain weights:\n",
    "    #weights = particles.getCauchyWeight()\n",
    "    weights = particles.getGaussianWeight()\n",
    "\n",
    "    # Create array of possible indices to resample:\n",
    "    allIndices = np.array(range(particles.numParticles))\n",
    "    \n",
    "    # Create histogram buckets based on the cumulative weights\n",
    "    cumulativeWeights = np.concatenate(([0.0], np.cumsum(weights)))\n",
    "    \n",
    "    # Find first starting position:\n",
    "    startPos = np.random.rand()/particles.numParticles\n",
    "    lengths = 1.0/particles.numParticles\n",
    "    #print startPos, lengths\n",
    "    selectionValues = allIndices*lengths + startPos\n",
    "    \n",
    "    # Create a histogram of selectionValues within the cumulativeWeights buckets\n",
    "    bucketValues, buckets = np.histogram(selectionValues, bins=cumulativeWeights)\n",
    "    \n",
    "    #newSampleIndices has now the number of times each index should be resampled\n",
    "    # We need to go from [0, 0, 1, 4, 0] to [2,3,3,3,3]\n",
    "    newSampleIndices = np.repeat(allIndices, bucketValues)\n",
    "    \n",
    "    # Return a new set of particles\n",
    "    return resampleParticles(particles, newSampleIndices, reinitialization_variance)\n",
    "\n",
    "    \n",
    "globalParticles.plotDistanceInfo(title=\"Initial particles\")\n",
    "\n",
    "stochasticUniversalParticles = stochasticUniversalSampling(globalParticles, reinitialization_variance=resample_variance)\n",
    "stochasticUniversalParticles.plotDistanceInfo(title=\"From Stochastic universal sampling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Monte Carlo Metropolis-Hasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mcMetropolisHastingSampling(particles,  reinitialization_variance=0):\n",
    "    # Obtain weights:\n",
    "    #weights = particles.getCauchyWeight()\n",
    "    weights = particles.getGaussianWeight()\n",
    "    \n",
    "    # Create buffer for indices which should be in the new ensemble:\n",
    "    newSampleIndices = np.zeros_like(weights, dtype=int)\n",
    "    \n",
    "    # The first member is automatically a member of the new ensemble\n",
    "    newSampleIndices[0] = 0\n",
    "    \n",
    "    # Iterate through all weights, and apply the Metropolis-Hasting algorithm\n",
    "    for i in range(1, particles.numParticles):\n",
    "        # Draw random number U[0,1]\n",
    "        p = np.random.rand()\n",
    "        if p < weights[i]/weights[newSampleIndices[i-1]]:\n",
    "            newSampleIndices[i] = i\n",
    "        else:\n",
    "            newSampleIndices[i] = newSampleIndices[i-1]\n",
    "    \n",
    "    # Return a new set of particles\n",
    "    return resampleParticles(particles, newSampleIndices, reinitialization_variance)\n",
    "\n",
    "    \n",
    "globalParticles.plotDistanceInfo(title=\"Initial particles\")\n",
    "\n",
    "metropolisHastingParticles = mcMetropolisHastingSampling(globalParticles, reinitialization_variance=resample_variance)\n",
    "metropolisHastingParticles.plotDistanceInfo(title=\"From Monte Carlo Metropolis-Hasting\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Na\u00efve drift trajectories in the SWE simulators\n",
    "\n",
    "Here, we will make a naive implementation of particles drifting within our simplified ocean models.\n",
    "For simplicity, a non-staggered implementation is chosen, as it makes it easier to evaluate the velocity field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DEFINE PARAMETERS\n",
    "\n",
    "#Coriolis well balanced reconstruction scheme\n",
    "nx = 50\n",
    "ny = 50\n",
    "\n",
    "dx = 4.0\n",
    "dy = 4.0\n",
    "\n",
    "dt = 0.1\n",
    "g = 9.81\n",
    "\n",
    "f = 0.5\n",
    "r = 0.0\n",
    "\n",
    "waterHeight = 10\n",
    "\n",
    "# WIND\n",
    "wind = Common.WindStressParams(type=99)\n",
    "\n",
    "ghosts = np.array([2,2,2,2]) # north, east, south, west\n",
    "validDomain = np.array([2,2,2,2])\n",
    "boundaryConditions = Common.BoundaryConditions(2,2,2,2)\n",
    "\n",
    "# Define which cell index which has lower left corner as position (0,0)\n",
    "x_zero_ref = 2\n",
    "y_zero_ref = 2\n",
    "\n",
    "dataShape = (ny + ghosts[0]+ghosts[2], \n",
    "             nx + ghosts[1]+ghosts[3])\n",
    "\n",
    "eta0 = np.zeros(dataShape, dtype=np.float32, order='C');\n",
    "u0 = np.zeros(dataShape, dtype=np.float32, order='C');\n",
    "v0 = np.zeros(dataShape, dtype=np.float32, order='C');\n",
    "\n",
    "# Bathymetry:\n",
    "Hi = np.ones((dataShape[0]+1, dataShape[1]+1), dtype=np.float32, order='C')*waterHeight\n",
    "\n",
    "# Add disturbance:\n",
    "addBump(eta0, nx, ny, dx, dy, 0.3, 0.5, 0.05, validDomain)\n",
    "addBump(eta0, nx, ny, dx, dy, 0.7, 0.2, 0.10, validDomain)\n",
    "addBump(eta0, nx, ny, dx, dy, 0.1, 0.8, 0.03, validDomain)\n",
    "eta0 = eta0*0.3\n",
    "\n",
    "#Calculate radius from center of bump for plotting\n",
    "x_center = dx*nx/2.0\n",
    "y_center = dy*ny/2.0\n",
    "y_coords, x_coords = np.mgrid[0:ny*dy:dy, 0:nx*dx:dx]\n",
    "#x_coords = np.subtract(x_coords, x_center)\n",
    "#y_coords = np.subtract(y_coords, y_center)\n",
    "radius = np.sqrt(np.multiply(x_coords, x_coords) + np.multiply(y_coords, y_coords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Define a bunch of particles to be released within the given domain\n",
    "numParticles = 50\n",
    "observation_variance = 5*dx\n",
    "\n",
    "constOceanParticles = GlobalParticles(numParticles, observation_variance)\n",
    "constOceanParticles.initializeInSquare(dx*nx, dy*ny)\n",
    "\n",
    "constOceanParticles.plotDistanceInfo(title=\"Initial particles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def applyPeriodicBoundaryConditionsToParticles(positions, nx, ny, dx, dy):\n",
    "    # Loop over particles\n",
    "    for i in range(positions.shape[0]):\n",
    "        x, y = positions[i,0], positions[i,1]\n",
    "        \n",
    "        # Check what we assume is periodic boundary conditions\n",
    "        if x < 0:\n",
    "            x = dx*nx + x\n",
    "        if y < 0:\n",
    "            y = dy*ny + y\n",
    "        if x > dx*nx:\n",
    "            x = dx*nx - x\n",
    "        if y > dy*ny:\n",
    "            y = dy*ny - y\n",
    "        \n",
    "        positions[i,0] = x\n",
    "        positions[i,1] = y\n",
    "\n",
    "def particleDrifter(positions, eta, hu, hv, H0, dt, nx, ny, dx, dy, x_zero_ref, y_zero_ref, \\\n",
    "                    sensitivity=1, doPrint=False):\n",
    "    # Change positions by reference\n",
    "    \n",
    "    numParticles = positions.shape[0]\n",
    "    # Loop over particles\n",
    "    for i in range(numParticles):\n",
    "        if doPrint: print \"---------- Particle \" + str(i) + \" ---------------\"\n",
    "        x0, y0 = positions[i,0], positions[i,1]\n",
    "        if doPrint: print \"(x0, y0): \", (x0,y0)\n",
    "        \n",
    "        # First, find which cell each particle is in\n",
    "        \n",
    "        # In x-direction:\n",
    "        cell_id_x = int(np.ceil(x0/dx) + x_zero_ref)\n",
    "        cell_id_y = int(np.ceil(y0/dy) + y_zero_ref)\n",
    "        \n",
    "        if doPrint: print \"cell values in x-direction: \", ((cell_id_x-2-0.5)*dx, (cell_id_x-2+0.5)*dx)\n",
    "        if doPrint: print \"cell values in y-direction: \", ((cell_id_y-2-0.5)*dy, (cell_id_y-2+0.5)*dy)\n",
    "        \n",
    "        h = waterHeight + eta[cell_id_y, cell_id_x]\n",
    "        u = hu[cell_id_y, cell_id_x]\n",
    "        v = hv[cell_id_y, cell_id_x]\n",
    "        \n",
    "        if doPrint: print \"Velocity: \", (u, v)\n",
    "        \n",
    "        x1 = sensitivity*u*dt + x0\n",
    "        y1 = sensitivity*v*dt + y0\n",
    "        if doPrint: print \"(x1, y1): \", (positions[i,0], positions[i,1])\n",
    "        \n",
    "        positions[i,0] = x1\n",
    "        positions[i,1] = y1\n",
    "        \n",
    "    \n",
    "    # Check what we assume is periodic boundary conditions    \n",
    "    applyPeriodicBoundaryConditionsToParticles(positions, nx, ny, dx, dy)\n",
    "        \n",
    "        \n",
    "#eta1, hu1, hv1 = sim.download()\n",
    "#particleDrifter(oceanParticles.positions, eta1, hu1, hv1, waterHeight, \\\n",
    "#                dt, nx, ny, dx, dy, x_zero_ref, y_zero_ref, \\\n",
    "#                doPrint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "oceanParticles = GlobalParticles(numParticles)\n",
    "oceanParticles.positions = constOceanParticles.positions.copy()\n",
    "\n",
    "#Clean up old simulator if any:\n",
    "if 'sim' in globals():\n",
    "    sim.cleanUp()\n",
    "    \n",
    "#Initialize simulator\n",
    "reload(CDKLM16)\n",
    "reload(PlotHelper)\n",
    "sim = CDKLM16.CDKLM16(cl_ctx, eta0, u0, v0, Hi, \\\n",
    "                nx, ny, dx, dy, dt, g, f, r, \\\n",
    "                boundary_conditions=boundaryConditions)\n",
    "\n",
    "fig = plt.figure()\n",
    "plotter = PlotHelper.PlotHelper(fig, x_coords, y_coords, radius, \n",
    "                                eta0[validDomain[2]:-validDomain[0], validDomain[3]:-validDomain[1]], \n",
    "                                u0[validDomain[2]:-validDomain[0], validDomain[3]:-validDomain[1]], \n",
    "                                v0[validDomain[2]:-validDomain[0], validDomain[3]:-validDomain[1]])\n",
    "\n",
    "plotter.showParticles(oceanParticles.getParticlePositions(),\n",
    "                      oceanParticles.getObservationPosition())\n",
    "\n",
    "T = 200\n",
    "sensitivity = 20\n",
    "loopsPerFrame = 10\n",
    "def animate(i):\n",
    "    if (i>0):\n",
    "        for j in range(loopsPerFrame):\n",
    "            t = sim.step(dt)\n",
    "            eta1, hu1, hv1 = sim.download()\n",
    "\n",
    "            particleDrifter(oceanParticles.positions, eta1, hu1, hv1, waterHeight, \\\n",
    "                            dt, nx, ny, dx, dy, x_zero_ref, y_zero_ref, sensitivity=sensitivity)\n",
    "\n",
    "    else:\n",
    "        t = 0.0\n",
    "\n",
    "    eta1, hu1, hv1 = sim.download()\n",
    "    plotter.plot(eta1[validDomain[2]:-validDomain[0], validDomain[3]:-validDomain[1]], \n",
    "                 hu1[validDomain[2]:-validDomain[0], validDomain[3]:-validDomain[1]], \n",
    "                 hv1[validDomain[2]:-validDomain[0], validDomain[3]:-validDomain[1]]);\n",
    "    \n",
    "    plotter.showParticles(oceanParticles.getParticlePositions(),\n",
    "                          oceanParticles.getObservationPosition())\n",
    "    \n",
    "    fig.suptitle(\"CDKLM16 Time = \" + \"{:04.0f}\".format(t) + \" s\", fontsize=18)\n",
    "\n",
    "    if (i%20 == 0):\n",
    "        print \"{:03.0f}\".format(100*i / T) + \" % => t=\" + str(t) + \"\\tMax eta: \" + str(np.max(eta1)) + \\\n",
    "        \"\\tMax hu: \" + str(np.max(hu1)) + \\\n",
    "        \"\\tMax hv: \" + str(np.max(hv1))\n",
    "        print \"\\t\\tObservation pos: \", oceanParticles.getObservationPosition()\n",
    "                     \n",
    "anim = animation.FuncAnimation(fig, animate, range(T), interval=100)\n",
    "plt.close(anim._fig)\n",
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print constOceanParticles.getObservationPosition()\n",
    "print oceanParticles.getObservationPosition()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4,4))\n",
    "posis = np.array([[0.1, 0.2, 0.3, 0.4],[0.2, 0.4, 0.6, 0.8]])\n",
    "print posis\n",
    "scat = plt.scatter(x=posis[0,:], y=posis[1,:])\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.grid()\n",
    "posis[0,:] = posis[0,:]*1.8\n",
    "print posis\n",
    "newData = np.hstack((xpath*2, ypath))\n",
    "scat.set_offsets(posis.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Sorting example from https://stackoverflow.com/a/21077060\n",
    "\n",
    "people = np.array(['Jim', 'Pam', 'Micheal', 'Dwight'])\n",
    "ages = np.array([27, 25, 4, 9])\n",
    "sorted_indices = ages.argsort()\n",
    "print people[sorted_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}